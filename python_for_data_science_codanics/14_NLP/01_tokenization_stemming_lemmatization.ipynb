{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing (NLP)**\n",
    "\n",
    "Natural language processing (NLP) is the application of computational methods to not only extract information from text but also model different applications on top of it. All language based text have systematic structure or rules which is often referred as morphology, `for example` the past tense of “jump” is always “ jumped”. For humans this morphological understanding is obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tokenization**\n",
    "\n",
    "The task of segmenting text into relevant words in called tokenization.\n",
    "\n",
    "In simplest form, tokenization can be achieved by splitting text using whitespace. \n",
    "\n",
    "`NLTK` provides a function called `word_tokenize()` for splitting strings into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'will',\n",
       " 'look',\n",
       " 'into',\n",
       " 'the',\n",
       " 'core',\n",
       " 'components',\n",
       " 'that',\n",
       " 'are',\n",
       " 'relevant',\n",
       " 'to',\n",
       " 'language',\n",
       " 'in',\n",
       " 'computational',\n",
       " 'linguistics']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'we will look into the core components that are relevant to language in computational linguistics'\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But simple tokenization doesn’t work all the time.\n",
    "- In case of complex words which involves punctuation marks in between words ( Example: what’s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', \"'s\", 'up', '?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'What\\'s up?'\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want to preserve that word with punctuations, simple hack is that we can split the text into words by white spaces and replace all punctuation with nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whats', 'up']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "text = 'What\\'s up?'\n",
    "words = text.split()\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "[w.translate(table) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stemming & Lemmatization**\n",
    "\n",
    "Task of reducing each word to its root . For example “Walk” is the root for words like “Walks”, “Walking”, “Walked”. Usually the root may hold significantly more meaning than the tense itself. So in NLP tasks it’s very important to extract the root for the words in the text.\n",
    "\n",
    "`Stemming` helps in reducing the vocabulary present in the documents, which saves a lot of computation. Also in the tasks like classification, tenses of words are rendered irrelevant once stemming is applied.\n",
    "\n",
    "Most popular method is the `Porter Stemming algorithm`. Its a Suffix stripping algorithms which do not rely on a lookup table that consists of inflected forms and root form relations. Some simple rules are built for extracting the root words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stemming vs Lemmatization**\n",
    "\n",
    "Stemming and lemmatization are both techniques used to reduce words to a common base form, but they differ in how they do it. `Stemming is faster, but lemmatization is more accurate.` \n",
    "\n",
    "The practical distinction between stemming and lemmatization is that, where `stemming` merely removes common suffixes from the end of word tokens, `lemmatization` ensures the output word is an existing normalized form of the word\n",
    "\n",
    "`lemmatization` does very similar to `stemming` as it removes inflection and suffixes to convert words into their root words. \n",
    "\n",
    "**`Meaning and context can be lost in the Stemming, lemmatization preserves the context.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'walk', 'walk', 'ate', 'eat', 'eat']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming Method\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "tokens = ['Walked', 'Walks', 'Walking', 'ate', 'eats', 'eating']\n",
    "[porter.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walked', 'Walks', 'Walking', 'ate', 'eats', 'eating']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization Method\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "tokens2 = ['Walked', 'Walks', 'Walking', 'ate', 'eats', 'eating']\n",
    "[lemma.lemmatize(word) for word in tokens2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normalisation Case**\n",
    "\n",
    "It is common to convert all words to one case\n",
    "\n",
    "### **Stop Words**\n",
    "\n",
    "Stop words are those words that do not contribute in the process of extracting/modelling on the text data because thery are the most common words such as: `the, a, and is`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note**\n",
    "`Data Cleaning:` Before applying complex computational methods on the text data, we are expected to understand and clean the data. These techniques help us make the text ready for modelling with advanced DNN and NLP techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
