{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NLP Engineers` often deal with corpus of documents or texts. \n",
    "\n",
    "`Raw Text` cannot be directly fed into the machine learning algorithms. It is very important to develop some methods to represent these documents in a way computers/algorithms understand i.e `vectors of numbers`. These methods are also called as `feature extraction methods or feature encoding`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag Of Words**\n",
    "\n",
    "This is very flexible, intuitive and easiest of feature extraction methods.\n",
    "\n",
    "The text/sentence is represented as a list of counts of unique words, for this reason this method is also referred as `count vectorisation`. To vectorize our documents, all we have to do is count how many time each words appears.\n",
    "\n",
    "**`Bag Of Words`** model weighs words based on occurence.\n",
    "\n",
    "**Note:** Remove Stop Words before doing count vectorization.\n",
    "\n",
    "**Vocabulary** is the total number of unique words in these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['the', 'sat', 'on', 'cat', 'mat', 'dog', 'log', 'dogs', 'and', 'cats', 'living', 'together']\n",
      "[[0. 2. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [\n",
    "    'The cat sat on the mat.',\n",
    "    'The dog sat on the log.',\n",
    "    'Dogs and cats living together.'\n",
    "]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "print(f'Vocabulary: {list(tokenizer.word_index.keys())}')\n",
    "\n",
    "# Convert the texts to a matrix\n",
    "text_matrix = tokenizer.texts_to_matrix(docs, mode='count')\n",
    "\n",
    "print(text_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Drawbacks**\n",
    "The model is only concerned with whether known words occur in the document, not where in the document. Obviously there is significant information loss by simply using a document vector to represent an entire document as the order or structure of words in the document is discarded, but this is sufficient for many computational linguistics applications. it computationally simpler and actively used when positioning or contextual info aren’t relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "\n",
    "TF-IDF is a method that provides a way to give rarer words greater weight.\n",
    "\n",
    "### **Term Frequency: tf(f,d)**\n",
    "\n",
    "This summarizes how often a given word appears within a document.\n",
    "\n",
    "It is measure of how frequently a word presents in a document.\n",
    "\n",
    "`2 Methods`\n",
    "\n",
    "1. Term frequency adjusted for document length:\n",
    "\n",
    "`tf(t,d) = (number of times term t appear in document d) ÷ (number of words in d)`\n",
    "\n",
    "2. Logarithmically scaled frequency:\n",
    "\n",
    "`tf(t,d) = log(1 + number of times term t appear in document d)`\n",
    "\n",
    "\n",
    "### **Inverse Document Frequency: idf(t,D)**\n",
    "\n",
    "IDF is a measure of term importance. \n",
    "\n",
    "It is logarithmically scaled ratio of the total number of documents vs the count of documents with term t.\n",
    "\n",
    "`idf(t,D) = log N / |{d belongs D: t belongs d}|`\n",
    "\n",
    "**Numerator:** Total number of documents\n",
    "\n",
    "**Denominator:** Total number of Documents with term\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "D = [ ‘a dog live in home’, ‘a dog live in the hut’, ‘hut is dog home’ ]   `# D is the corpus`\n",
    "\n",
    "idf(dog, D) = log( total number of documents (3) / total number of documents with term “dog” (3) ) = log(3/3) = log(1) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TFIDF: tf x idf**\n",
    "\n",
    "`tfidf(t,d,D) = tf(t,d) . idf(t,D)`\n",
    "\n",
    "We can now compute the TF-IDF score for each term in a document. \n",
    "\n",
    "Score implies the importance of the word.\n",
    "As you can see in the above example. If the term “dog” appears in all the documents, then the inverse document frequency of the word will be zero, thus the TFIDF score will be zero. What this basically implies is that if the same word is present in all the documents, then it has no relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['dog', 'live', 'in', 'home', 'hut', 'is']\n",
      "N/n: [1.         1.28768207 1.28768207 1.28768207 1.69314718 1.28768207] \n",
      "\n",
      "idf = log(N/n): {'dog': 0, 'live': 5, 'in': 3, 'home': 1, 'hut': 2, 'is': 4} \n",
      "\n",
      "[[0.40912286 0.52682017 0.         0.52682017 0.         0.52682017]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    'a dog live in home',\n",
    "    'a dog live in hut',\n",
    "    'hut is dog home'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "print(f'Vocabulary: {list(vectorizer.vocabulary_.keys())}')\n",
    "print('N/n:', vectorizer.idf_ , '\\n')\n",
    "print('idf = log(N/n):', vectorizer.vocabulary_, '\\n')\n",
    "\n",
    "vector = vectorizer.transform([docs[0]])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Drawbacks**\n",
    "TF-IDF makes the feature extraction more robust than just counting the number of instances of a term in a document as presented in Bag-of-words model. But it doesn’t solve for the major drawbacks of BoW model, the order or structure of words in the document is still discarded in TF-IDF model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note**\n",
    "\n",
    "**Sparsity:** As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them). NLP practitioners usually apply principal component analysis (PCA) to reduce the dimensionality.\n",
    "\n",
    "**Naive Bayes Models:** An over-simplified assumptions model, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification with BoW model or TF-IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
