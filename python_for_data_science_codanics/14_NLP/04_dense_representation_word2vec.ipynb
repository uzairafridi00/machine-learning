{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already understood how to create sparse vector like `TF-IDF` and `BoW` for a document with dimensions corresponding to words in the vocabulary of corpus (This dimension will be huge)\n",
    "\n",
    "`Sparse` means having too many zeros in vector representations.\n",
    "\n",
    "There are some serious drawbacks with these long sparse vector representations like `TF-IDF` and `BoW`.\n",
    "\n",
    "1. Large memory and expensive computation because the vectors are long.\n",
    "2. Significant memory loss as order of words in the documents is irrelevant.\n",
    "3. Hard to model as the number of model parameters to train will be in the scale of input vector length which is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will see how to tackle these problems using dimensionality reduction techniques and importantly deep learning. \n",
    "\n",
    "Using different techniques, we will extract powerful word representations called `embeddings (Dense, short vectors)`. Unlike the TFIDF or BoW, these vectors length is in the range of 50–300. These vectors work better in every NLP problem than sparse vectors as order/structure of words play a major role. So similar meaning words have similar representations.\n",
    "\n",
    "`For example:` “boat” and “ship” mean two different things in sparse vector representations, but embedding succeed in capturing the similarity between these words. \n",
    "\n",
    "There are 2 most popular and opensources embedding models `Word2Vec` and `GLoVe`. The word2vec methods are fast, efficient to train, and easily available online with static code and pretrained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word2Vec**\n",
    "\n",
    "These embedding are so powerful that vector representation of queen is very similar to that of v(king) − v(man) + v(woman). These representations are powerful in capturing syntactic relationships.\n",
    "\n",
    "The proposed architectures consisted of the continuous `bag-of-words (CBOW)` model and the `skip-gram` model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CBOW (Continuous bag-of-words)**\n",
    "\n",
    "Word used in similar ways/context result in similar representations. \n",
    "\n",
    "`For Example:` Synonyms like sad and unhappy are used in similar context. But how do we define context?\n",
    "\n",
    "![CBOW](./images/cbow.png)\n",
    "\n",
    "The neighbouring words will give us the context of the target word which is “sad” in the above example. So here context is simply window of c words to the left and right side of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification Problem Setting**\n",
    "\n",
    "In line with the intuition described, we will try to predict the current target word (“sad”) based on the context words (surrounding words). The number of surrounding words to consider for predicting is called context window. \n",
    "\n",
    "For above example, if context window equals 2 then the train data will be ([“would”,“be”, “memory”, “to”],[“sad”]). So if you observe closely this neural architecture is unsupervised, All we need to give is huge corpus(set of all documents) nothing more than that. It can create X(input is surrounding variables ), y (target word) in rolling manner as shown in the below diagram and it can construct dense word embeddings from the corpus.\n",
    "\n",
    "![CBOW](./images/cbow_1.png)\n",
    "\n",
    "Once the `X: input/context words` and `y: output/target` words are created from the corpus as described, the immediate task to design a model that does classification for us where we try to predict a target word from the context words.\n",
    "\n",
    "![CBOW](./images/cbow_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Architecture**\n",
    "\n",
    "There are 3 important aspects this neural architecture, Input layer, lambda layer/averaging layer and dense softmax. The most important and also confusing component is input layer. Input layer is often called as `embedding layer`.\n",
    "\n",
    "Let’s say we have a vocabulary of N words and we plan to get a dense vector of size K. The input layer maps each context word through an embedding matrix N to a dense vector representation of dimension K, so it is a NxK matrix where each word has a respective K sized vector.\n",
    "\n",
    "![Neural Network Architecture](./images/nn_architecture.png)\n",
    "\n",
    "![Neural Network Architecture](./images/nn_architecture_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CBOW Training**\n",
    "\n",
    "The embedding layer is randomly initialised and all the numbers in the embedding layer are trainable parameters. So the embedding layer gets better and better as more data is fed into the model.\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "Log of conditional probability of target word given context words. We match the predicted word with the actual target word, compute the loss by leveraging the categorial cross entropy loss and perform backpropagation with each epoch to update the embedding layer in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/nerd-for-tech/nlp-zero-to-one-count-based-embeddings-glove-part-6-40-c5bb3ebfd081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
