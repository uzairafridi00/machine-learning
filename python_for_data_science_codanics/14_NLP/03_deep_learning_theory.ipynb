{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Network**\n",
    "\n",
    "The idea of `neural networks` draws its inspiration from the `biological neurons of the human brain`. \n",
    "\n",
    "Neural network is a network of small computing units, each of which takes a vector of input values “X” and outputs a single value “y”. Neural nets often referred as deep learning, because these networks have many layers of small computing units. In this blog we will introduce basic neural networks every NLP practitioner should know.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Basic Computing Unit: Perceptron Algorithm**\n",
    "\n",
    "![Perceptron](images/perceptron.png)\n",
    "\n",
    "`Deep learning is combining a network of artificial neurons where information is passed between neurons. Each of these neurons learns a different function from its input vector and output a single value.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Perceptron**\n",
    "\n",
    "Perceptron is the basic computing unit of neuron.\n",
    "\n",
    "The basic form of the perceptron algorithm is same mathematics as logistic regression.\n",
    "\n",
    "![Perceptron](images/perceptron_2.png)\n",
    "\n",
    "`b` is the learnable bias term. We can treat b as an additional weight w₀.\n",
    "\n",
    "Individually learned weight wᵢ is multiplied with xᵢ of X and passed to a function f(.) to get output y. f(.) is called activation function. It is important to note that bias term or w₀ is also learnable parameters. This bias term allows the model to move the decision boundary away from the origin at perceptron level.\n",
    "\n",
    "![Perceptron](images/perceptron_3.png)\n",
    "\n",
    "As you can clearly see that perceptron assumes a linear relationship between input variables X and the output y. In real world problem, these linear assumptions of perceptron often fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feed-Forward, Fully Connected NN**\n",
    "\n",
    "1. Links these neurons together into different layers of a network.\n",
    "2. Uses a differentiable, non-linear activation function in each neuron.\n",
    "\n",
    "The NN is composed of interconnected neurons and the data flows in only direction so it is called feed-forward neural network. Layer is defined as a set of neurons. These layers are `fully connected`, meaning that each neuron in each layer takes as input the outputs from all the neurons in the previous layer, and there is a link between every pair of neurones from two adjacent layers. \n",
    "\n",
    "`NN must contain an input and output layer and at least one hidden layer.`\n",
    "\n",
    "![Perceptron](images/perceptron_4.png)\n",
    "\n",
    "- h1 and h2 are the nodes in the `hidden layer`. Observe that each node in hidden layer is completely connected to the input X.\n",
    "- `Non-linear Activation functions` are applied at the end of each neuron which allows the output value to be a non-linear, weighted combination of its inputs, thereby creating nonlinear features used by the next layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Non-Linear Activations Functions**\n",
    "\n",
    "In a node, a basic computation block in the neural-net activation functions are used to ensure the output value to be a non-linear, weighted combination of its inputs. \n",
    "\n",
    "Non-linear activation functions play a very important role in improving the representative power of the neural-net. \n",
    "\n",
    "We’ll discuss three popular non-linear functions f(.).\n",
    "\n",
    "![Perceptron](images/perceptron_5.png)\n",
    "\n",
    "### 1. **Sigmoid**\n",
    "- With range (0,1), this function acts as a continuous squashing function. It also have continuous derivative ideal for gradient descent methods.\n",
    "- **Drawbacks:** Internal covariate shift is introduced because the outputs of the sigmoid are not cantered around 0, but instead around 0.5 as this introduces a discrepancy between the layers because the outputs are not in a consistent range.\n",
    "\n",
    "### 2. **Tanh**\n",
    "- With range(−1,1), this is it is zero-centered function which solves one of the issues with the sigmoid activation function.\n",
    "- **Drawbacks:** the gradient saturation at the extremes of the function, this will cause the gradients to be very close to 0.\n",
    "\n",
    "### 3. **ReLU**\n",
    "- with range(0,♾), simple, fast activation function typically found in computer vision. The function is a linear, if input is greater than 0.\n",
    "- ReLU function is computationally very faster as sigmoid and tanh functions require exponential operation. It promises better convergence because of non-saturating gradient in one direction.\n",
    "- **Drawbacks:** Relu must be used very carefully if large gradients are involved, large gradient update prevents the neuron from ever updating again. So learning rated must be kept lower when dealing with Relu.\n",
    "\n",
    "### 4. **Leaky ReLU**\n",
    "- Leaky ReLU introduces α parameter that allows small gradients to be back-propagated.\n",
    "  \n",
    "### 5. **Softmax**\n",
    "- The softmax function allows us to output a categorical probability distribution over K classes. We can use the softmax to produce a vector of probabilities according to the output of that neuron"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
