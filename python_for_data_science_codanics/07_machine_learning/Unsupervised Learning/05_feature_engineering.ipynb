{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- Data is new `oil`.\n",
    "- `Structure data` > Dataframe which contains Rows and Columns.\n",
    "- `Features` (X) are those columns which are input variables to train a model to predict something (y).\n",
    "- Features = Inputs = X = Predictors.\n",
    "\n",
    "### Feature Engineering\n",
    " [https://www.youtube.com/watch?v=Vst6RXyKUIw]\n",
    "\n",
    "Remember these 5 main steps for Feature Engineering:\n",
    "\n",
    "1. `Feature Checking`.\n",
    "   1. Is there any anomaly.\n",
    "   2. Outliers.\n",
    "   3. Missing values.\n",
    "   4. Brain Storming on Features. Know your features first.\n",
    "   5. Composition > Distribution > Relationship > Comparison\n",
    "   6. Know these 4 about features.\n",
    "2. `Create new features if you need for predictions`.\n",
    "3. `How these features contribute to each models training`.\n",
    "   1. Ranking of features.\n",
    "4. `Re-iteration:` creating new features based on ML models.\n",
    "5. `Feature Selection`.\n",
    "6. Model Computation.\n",
    "7. Resources.\n",
    "\n",
    "## Notes:\n",
    "\n",
    "When we get the data, we do: \n",
    "\n",
    "1. Domain Knowledge\n",
    "2. Data Pre-processing\n",
    "   1. Handling Missing Data\n",
    "   2. Outlier Detections\n",
    "3. Dealing with Numerical Features\n",
    "   1. Cleaning, Splitting and Converting\n",
    "   2. Normalize (Log Transformer) or Scale the data\n",
    "   3. Handling Skewness\n",
    "   4. Binning (Age Grouping)\n",
    "   5. Create new columns\n",
    "4. Dealing with Categorical Features\n",
    "   1. Type of Categorical Features\n",
    "      1. Nominal\n",
    "         1. Binary\n",
    "      2. Ordinal\n",
    "   2. Encoding based on categorical type\n",
    "      1. Label Encoding\n",
    "      2. One Hot Encoding\n",
    "      3. Target Encoding\n",
    "      4. Binary Encoding\n",
    "5. Date and Time features\n",
    "   1. Reformat\n",
    "   2. Split Date and Time\n",
    "   3. Convert to same unit\n",
    "   4. Binning\n",
    "      1. Summer Time\n",
    "      2. Winter Time\n",
    "   5. 12:45 (GMT+2) - 13:40 (GMT+3) > Subtract and create new column. Time of flight.\n",
    "   6. Cyclic Time Calculation > Time and Days calculations\n",
    "   7. Months > Leap Year (Feb makes issue)\n",
    "   8. Days of Week\n",
    "6. Text features (used in NLP)\n",
    "   1. Know NLP Basics\n",
    "      1. Tokenization\n",
    "      2. Vectorize\n",
    "      3. TF-IDF\n",
    "      4. Bag of Words\n",
    "      5. Words Embedding\n",
    "      6. Topic Modelling\n",
    "7. Image features (used in Computer Vision)\n",
    "   1. Binary Color\n",
    "   2. RGB Color\n",
    "   3. Tensorflow library is used.\n",
    "8. Feature Selection Techniques\n",
    "   1. Filter\n",
    "   2. Wrapper\n",
    "   3. Embedded\n",
    "   4. Dimensionality Reduction (Dealing with Multicollinearity)\n",
    "9.  Advanced Feature Engineering Techniques\n",
    "10. Real-time/ Real Life online Data (for projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection**\n",
    "[https://www.youtube.com/watch?v=CtLyMRaZ65o]\n",
    "\n",
    "Below are the methods for feature selection:\n",
    "\n",
    "1. `Filter Method`\n",
    "   1. Basic Statistical Analysis\n",
    "   2. Correlation - Pearson and Spearman's\n",
    "   3. Chi-Square (Categorical)\n",
    "   4. ANOVA - F-value\n",
    "   5. Variance\n",
    "   6. Mean Comparison\n",
    "2. `Wrapper Method` \n",
    "   1. Use ML Algorithms.\n",
    "   2. Add 1st and 2nd feature and look for metrics. Then 2nd and 3rd, 3rd and 4th. And we look for high accuracy. This is called `Forward Selection`.\n",
    "   3. `Backward Elimination` -> Same like forward but in backward we eliminate the feature.\n",
    "   4. `Recrusive Feature Elimination (RFE)` -> We make our own combination to check the feature accuracy.\n",
    "   5. `Exhaustive Feature Selection` -> Check all the possible combinations and select best accuracy.\n",
    "3. `Embedded Method`\n",
    "   1. Combination of Filter + Wrapper methods.\n",
    "   2. We do feature selection during model training in this method.\n",
    "   3. Algorithms are Decision Tree / Random Forest, GBMS (Gradient Boosting Machines)\n",
    "4. `Dimensionality Reduction`\n",
    "   1. It's hard to compute multiple dimensions data. So we reduce the dimensions to compute it easily.\n",
    "   2. PCA (Principal Compnent Analysis)\n",
    "   3. SVD (Singular Value Decomposition)\n",
    "   4. t-SNE (t-distributed Stochastic Neighbour Embedding)\n",
    "   5. LDA (Linear Discriminate Anaysis)\n",
    "   6. NMF (Non-Negative Matrix Factorization)\n",
    "5. `Regularization method`\n",
    "   1. Lasso and Ridge Regression (L1 & L2)\n",
    "   2. Elastic Net Regularization\n",
    "6. `Deep Learning Based Feature Selection`\n",
    "   1. Autoencoders\n",
    "   2. Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
