{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PCA (Principal Component Analysis)**\n",
    "\n",
    "PCA (Principal Component Analysis) is primarily used to reduce the `dimensionality` of large datasets by extracting the most important features, allowing for easier visualization and analysis of complex data, especially when dealing with high-dimensional information, while still preserving the key patterns and relationships within the data.\n",
    "\n",
    "It's commonly applied in data preprocessing for machine learning tasks like clustering and classification.\n",
    "\n",
    "1. We select one point of view and on that point of view it divide our data.\n",
    "   1. Size based (PC1) -> differentiate 90% of data \n",
    "   2. Shape based (PC2) -> differentiate 5% of data\n",
    "   3. And the other PCs are 2.5% , 2%, 0.5% variations and so on.\n",
    "2. Draw Scree plot -> Right Skewed. As this will tell us that which PCs are variating data more.\n",
    "3. Why PC1 distance matters alot as compare to PC2?\n",
    "   1. Because PC1 has 90% variation in data. PC2 has less variation. Other PCs are supporting the PC1 that YES there is difference.\n",
    "4. You can work wiht `Multivariate Analysis` using PCA.\n",
    "5. You `MUST SCALE` your Data before implementing PCA.\n",
    "6. It simplify the data. Easy to visualize.\n",
    "7. It reduce dimensionality of data.\n",
    "\n",
    "## Key Terms in PCA\n",
    "\n",
    "- `Dimensions` -> Features/Columns/Features.\n",
    "- `Principal Component`\n",
    "  - 37:00 \n",
    "- `Covariance Matrix`\n",
    "  - Variance & Covariance\n",
    "- `Eigen Vectors`\n",
    "- `Eigen Values`\n",
    "- Scree Plot\n",
    "- Loading Score\n",
    "- Biplots"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
