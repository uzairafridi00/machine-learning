{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's first understand how to generate n-grams using CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uzair': 5, 'is': 1, 'looking': 3, 'for': 0, 'ml': 4, 'job': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer()\n",
    "v.fit([\"Uzair is looking for ML job.\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uzair': 9,\n",
       " 'is': 2,\n",
       " 'looking': 5,\n",
       " 'for': 0,\n",
       " 'ml': 7,\n",
       " 'job': 4,\n",
       " 'uzair is': 10,\n",
       " 'is looking': 3,\n",
       " 'looking for': 6,\n",
       " 'for ml': 1,\n",
       " 'ml job': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range=(1, 2))\n",
    "v.fit([\"Uzair is looking for ML job.\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uzair': 12,\n",
       " 'is': 3,\n",
       " 'looking': 7,\n",
       " 'for': 0,\n",
       " 'ml': 10,\n",
       " 'job': 6,\n",
       " 'uzair is': 13,\n",
       " 'is looking': 4,\n",
       " 'looking for': 8,\n",
       " 'for ml': 1,\n",
       " 'ml job': 11,\n",
       " 'uzair is looking': 14,\n",
       " 'is looking for': 5,\n",
       " 'looking for ml': 9,\n",
       " 'for ml job': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range=(1, 3))\n",
    "v.fit([\"Uzair is looking for ML job.\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will not take a simple collection of text documents, preprocess them to remove stop words, lemmatize etc and then generate bag of 1 grams and 2 grams from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Thor ate pizza\",\n",
    "    \"Loki is tall\",\n",
    "    \"Loki is eating pizza\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load the english language model and create nlp object from it\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    # remove the stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thor eat pizza'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Thor ate pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loki tall'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Loki is tall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thor eat pizza', 'Loki tall', 'Loki eat pizza']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_processed = [\n",
    "    preprocess(text) for text in corpus\n",
    "]\n",
    "corpus_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 7,\n",
       " 'eat': 0,\n",
       " 'pizza': 5,\n",
       " 'thor eat': 8,\n",
       " 'eat pizza': 1,\n",
       " 'loki': 2,\n",
       " 'tall': 6,\n",
       " 'loki tall': 4,\n",
       " 'loki eat': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range=(1,2))\n",
    "v.fit(corpus_processed)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now generate bag of n gram vector for few sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Thor eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a document that has out of vocabulary (OOV) term and see how bag of ngram generates vector out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Hulk eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **News Category Classification Problem**\n",
    "\n",
    "Okay now that we know basics of BAG of n grams vectorizer ðŸ˜Ž It is the time to work on a real problem. Here we want to do a news category classification. We will use bag of n-grams and traing a machine learning model that can categorize any news into one of the following categories,\n",
    "\n",
    "- BUSINESS\n",
    "- SPORTS\n",
    "- CRIME\n",
    "- SCIENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12695, 2)\n",
      "                                                text  category\n",
      "0  Watching SchrÃ¶dinger's Cat Die University of C...   SCIENCE\n",
      "1     WATCH: Freaky Vortex Opens Up In Flooded Lake    SCIENCE\n",
      "2  Entrepreneurs Today Don't Need a Big Budget to...  BUSINESS\n",
      "3  These Roads Could Recharge Your Electric Car A...  BUSINESS\n",
      "4  Civilian 'Guard' Fires Gun While 'Protecting' ...     CRIME\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"./datasets/news_dataset.json\")\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check category distribution in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "BUSINESS    4254\n",
       "SPORTS      4167\n",
       "CRIME       2893\n",
       "SCIENCE     1381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle class imbalance\n",
    "\n",
    "As you can see above, SCIENCE category has almost 1/3rd data samples compared to BUSINESS and SPORTS categories. I initially trained a model without handling the imbalanced I saw a lower f1-score for SCIENCE category. Hence we need to address this imbalanced.\n",
    "\n",
    "Out of those techniques, I will use **undersampling** technique here.\n",
    "\n",
    "In undersampling, we take a minor class and sample those many samples from other classes, this means we are not utilizing all the data samples for training and in ML world - Not using all the data for training is considered a SIN! ðŸ˜µ In real life, you are advised to use a technique such as SMOTE so that you can utilize all of your dataset for the training but since this tutorial is more about bag of n-grams then class imbalance itself, I'd go with a simple technique of undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have these many SCIENCE articles and SCIENCE is our minority class\n",
    "min_sample = 1381\n",
    "\n",
    "df_BUSINESS = df[df.category == \"BUSINESS\"].sample(min_sample, random_state=42)\n",
    "df_SPORTS = df[df.category == \"SPORTS\"].sample(min_sample, random_state=42)\n",
    "df_CRIME = df[df.category == \"CRIME\"].sample(min_sample, random_state=42)\n",
    "df_SCIENCE = df[df.category == \"SCIENCE\"].sample(min_sample, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "BUSINESS    1381\n",
       "SPORTS      1381\n",
       "CRIME       1381\n",
       "SCIENCE     1381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced = pd.concat([df_BUSINESS, df_SPORTS, df_CRIME, df_SCIENCE], axis=0)\n",
    "df_balanced.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Convert text category to a number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {\"BUSINESS\": 0, \"SPORTS\": 1, \"CRIME\": 2, \"SCIENCE\": 3}\n",
    "\n",
    "df_balanced['category_num'] = df_balanced.category.map(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12166</th>\n",
       "      <td>U.S. Women's 4x100 Relay Team Scores Gold Afte...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5445</th>\n",
       "      <td>Report: New York Police Recruiting Muslim Info...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8801</th>\n",
       "      <td>Centuries-Old 'Meditating' Mummy Found</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6150</th>\n",
       "      <td>New Evidence of Prehistoric Trade With Asia Fo...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>Researchers Find More Women Buried At Stonehen...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text category  \\\n",
       "12166  U.S. Women's 4x100 Relay Team Scores Gold Afte...   SPORTS   \n",
       "5445   Report: New York Police Recruiting Muslim Info...    CRIME   \n",
       "8801             Centuries-Old 'Meditating' Mummy Found   SCIENCE   \n",
       "6150   New Evidence of Prehistoric Trade With Asia Fo...  SCIENCE   \n",
       "4828   Researchers Find More Women Buried At Stonehen...  SCIENCE   \n",
       "\n",
       "       category_num  \n",
       "12166             1  \n",
       "5445              2  \n",
       "8801              3  \n",
       "6150              3  \n",
       "4828              3  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Build a model with original text (no pre processing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced.text,\n",
    "    df_balanced.category_num,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_balanced.category_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4419,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6414     Arby's Employee Keeps Job After Refusing To Se...\n",
       "1318     Colorful NASA Image Shows Off Plutoâ€™s Psychede...\n",
       "4170     Women in Business Q&A: Sophie Delafontaine, Ar...\n",
       "11310    5 Formalized Referral Systems to Grow Your Sal...\n",
       "4188     Hawaii's Kilauea Volcano Sees A Mesmerizing Ri...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_num\n",
       "0    1105\n",
       "3    1105\n",
       "1    1105\n",
       "2    1104\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_num\n",
       "2    277\n",
       "0    276\n",
       "1    276\n",
       "3    276\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Attempt 1 : Use 1-gram which is nothing but a Bag Of Words (BOW) model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.84       276\n",
      "           1       0.92      0.85      0.88       276\n",
      "           2       0.91      0.89      0.90       277\n",
      "           3       0.89      0.82      0.85       276\n",
      "\n",
      "    accuracy                           0.87      1105\n",
      "   macro avg       0.88      0.87      0.87      1105\n",
      "weighted avg       0.88      0.87      0.87      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow', CountVectorizer(ngram_range=(1,1))),\n",
    "    ('Multi NB', MultinomialNB())\n",
    "])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12446    Shocking Video Of Officer Punching Woman Ignit...\n",
       "3868     YOLO ATTACK: Birthday Stabbing Turns Party Int...\n",
       "3301                             Great News For Obamacare \n",
       "11543    Katie Nolan Calls On Dallas Cowboys To Get Hel...\n",
       "9501     Markets Tumble On Fears Of Global Economic Slo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 1, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12446    2\n",
       "3868     2\n",
       "3301     0\n",
       "11543    1\n",
       "9501     0\n",
       "Name: category_num, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Attempt 2 : Use 1-gram and bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.95      0.82       276\n",
      "           1       0.92      0.83      0.87       276\n",
      "           2       0.91      0.88      0.89       277\n",
      "           3       0.93      0.79      0.85       276\n",
      "\n",
      "    accuracy                           0.86      1105\n",
      "   macro avg       0.87      0.86      0.86      1105\n",
      "weighted avg       0.87      0.86      0.86      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer_1_2_grams', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('Multi NB', MultinomialNB())\n",
    "])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Attempt 3 : Use 1-gram to trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.95      0.81       276\n",
      "           1       0.92      0.80      0.85       276\n",
      "           2       0.91      0.88      0.89       277\n",
      "           3       0.92      0.77      0.84       276\n",
      "\n",
      "    accuracy                           0.85      1105\n",
      "   macro avg       0.86      0.85      0.85      1105\n",
      "weighted avg       0.86      0.85      0.85      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer_1_2_grams', CountVectorizer(ngram_range=(1,3))),\n",
    "    ('Multi NB', MultinomialNB())\n",
    "])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Use text pre-processing to remove stop words, punctuations and apply lemmatization**\n",
    "  \n",
    "You may wonder, we have not done any text-processing yet to remove stop words, punctuations, apply lemmatization etc. Well we wanted to train the model without any preprocessing first and check the performance. Now we will re-do same thing but with preprocessing of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['preprocessed_txt'] = df_balanced['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_num</th>\n",
       "      <th>preprocessed_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>How to Develop the Next Generation of Innovato...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>develop Generation Innovators stop treat way g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>Madoff Victims' Payout Nears $7.2 Billion, Tru...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>Madoff Victims Payout near $ 7.2 billion Trust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7447</th>\n",
       "      <td>Bay Area Floats 'Sanctuary In Transit Policy' ...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>Bay Area Floats Sanctuary Transit Policy prote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10388</th>\n",
       "      <td>Microsoft Agrees To Acquire LinkedIn For $26.2...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>Microsoft agree acquire linkedin $ 26.2 billio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>Inside A Legal, Multibillion Dollar Weed Market</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "      <td>inside Legal Multibillion Dollar Weed Market</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category  \\\n",
       "594    How to Develop the Next Generation of Innovato...  BUSINESS   \n",
       "3093   Madoff Victims' Payout Nears $7.2 Billion, Tru...  BUSINESS   \n",
       "7447   Bay Area Floats 'Sanctuary In Transit Policy' ...  BUSINESS   \n",
       "10388  Microsoft Agrees To Acquire LinkedIn For $26.2...  BUSINESS   \n",
       "1782    Inside A Legal, Multibillion Dollar Weed Market   BUSINESS   \n",
       "\n",
       "       category_num                                   preprocessed_txt  \n",
       "594               0  develop Generation Innovators stop treat way g...  \n",
       "3093              0  Madoff Victims Payout near $ 7.2 billion Trust...  \n",
       "7447              0  Bay Area Floats Sanctuary Transit Policy prote...  \n",
       "10388             0  Microsoft agree acquire linkedin $ 26.2 billio...  \n",
       "1782              0       inside Legal Multibillion Dollar Weed Market  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Build a model with pre processed text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced.preprocessed_txt, \n",
    "    df_balanced.category_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df_balanced.category_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4419,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5230    fairy Witches Astronauts boy run away girl lik...\n",
       "2111    anticipation psychology wait Line spend lot ti...\n",
       "7443           Jake Snake Roberts Intensive Care Collapse\n",
       "1631    Jeweler order pay $ 34,500 Trashing Rival Fake...\n",
       "7066    7 kill Australia Worst Mass Shooting 1996 desc...\n",
       "Name: preprocessed_txt, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_num\n",
       "3    1105\n",
       "2    1105\n",
       "0    1105\n",
       "1    1104\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_num\n",
       "1    277\n",
       "0    276\n",
       "3    276\n",
       "2    276\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85       276\n",
      "           1       0.93      0.82      0.87       277\n",
      "           2       0.84      0.92      0.88       276\n",
      "           3       0.90      0.82      0.86       276\n",
      "\n",
      "    accuracy                           0.86      1105\n",
      "   macro avg       0.87      0.86      0.86      1105\n",
      "weighted avg       0.87      0.86      0.86      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow', CountVectorizer(ngram_range = (1, 2))), #using the ngram_range parameter \n",
    "    ('Multi NB', MultinomialNB())\n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Plot Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[248,   8,  10,  10],\n",
       "       [ 16, 228,  21,  12],\n",
       "       [ 16,   4, 254,   2],\n",
       "       [ 30,   5,  16, 225]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
