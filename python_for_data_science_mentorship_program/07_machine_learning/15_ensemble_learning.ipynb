{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Ensemble learning is a machine learning technique that combines the predictions of multiple models to make a final prediction. The idea behind ensemble learning is that by combining the predictions of multiple models, we can improve the accuracy and robustness of the final prediction.\n",
    "\n",
    "There are several ways to combine the predictions of multiple models, such as:\n",
    "\n",
    "- **Voting**: In voting, we make a final prediction by taking a majority vote of the predictions of multiple models. For example, if we have three models that predict the class labels \"cat\", \"dog\", and \"cat\", respectively, then the final prediction is \"cat\".\n",
    "\n",
    "- **Averaging**: In averaging, we make a final prediction by taking the average of the predictions of multiple models. For example, if we have three models that predict the values 1.2, 2.3, and 1.5, respectively, then the final prediction is (1.2 + 2.3 + 1.5) / 3 = 1.7.\n",
    "\n",
    "- **Stacking**: In stacking, we make a final prediction by training a new model on the predictions of multiple models. For example, we can train a logistic regression model on the predictions of a decision tree, random forest, and support vector machine, and use the logistic regression model to make a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ensemble Learning](./images/ensemble_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Ensemble Learning\n",
    "\n",
    "1. **Voting:**\n",
    "   - Works on majority count.\n",
    "   - Voting ensemble is a type of ensemble learning that combines the predictions of multiple models using a voting scheme. The voting scheme can be either hard voting or soft voting.\n",
    "\n",
    "2. **Bagging (Bootstrap Aggregation):**\n",
    "   - Helps to reduce variance and avoid overfitting.\n",
    "   - Creating multiple subsets of the original dataset with replacement (**Bootstraping**).\n",
    "   - Training a model on each subset.\n",
    "   - and then aggregating their predictions.\n",
    "   - A common example is the **Random Forest Algorithm**. \n",
    "  \n",
    "3. **Boosting:**\n",
    "   - Boosting is a type of ensemble learning that involves training multiple models in sequence, where each model is trained to correct the errors of the previous models. For example, we can use boosting to create multiple decision trees, where each tree is trained to correct the errors of the previous trees. This can reduce the bias of the final prediction, and improve the generalization performance of the model.\n",
    "   - Examples include **AdaBoost, Gradient Boosting, and XGBoost**.\n",
    "   - Boosting focuses on turning weak learners into strong ones interatively.\n",
    "\n",
    "4. **Stacking (Stacked Generalization):**\n",
    "   - Stacking is a type of ensemble learning that involves training multiple models on the same training data, and combining their predictions using a new model. For example, we can use stacking to train a decision tree, a random forest, and a support vector machine on the same training data, and combine their predictions using a logistic regression model. This can improve the accuracy and robustness of the final prediction, and is widely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Idea\n",
    "\n",
    "Ensemble learning works on wisdom of crowd. Different Models/knowledge combine to predict a better/accurate results.\n",
    "\n",
    "## Why to Use\n",
    "\n",
    "- Higher Accuracy.\n",
    "- Stability of the model, as they are less prone to errors.\n",
    "- Reduce overfitting.\n",
    "\n",
    "Used for disease predictions, recommendation systems.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- Complexity\n",
    "- Computational Expensive.\n",
    "- Harder to interpret due to complexity.\n",
    "- Requires careful parameters tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
