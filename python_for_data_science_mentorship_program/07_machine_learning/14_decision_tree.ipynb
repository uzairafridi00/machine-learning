{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Decision Tree is a type of supervised learning algorithm that is used for both classification and regression tasks. It is a tree-like model that uses a series of questions or conditions to split the data into subsets based on the values of the features.\n",
    "\n",
    "The decision tree algorithm works by recursively partitioning the data into subsets based on the values of the features. At each step, the algorithm selects the feature and the threshold that maximally reduces the impurity of the subsets. The impurity is measured using a criterion such as entropy or Gini index.\n",
    "\n",
    "The elements of the decision tree are:\n",
    "\n",
    "1. Node\n",
    "2. Root Node\n",
    "3. Splitting\n",
    "4. Decision Node\n",
    "5. Internal Node\n",
    "6. Leaf Node or Terminal Node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy is a measure of the impurity of a set of data, and is used in decision trees to determine the best split of the data. `For example`, if we have a set of data with two classes, and the classes are evenly distributed, then the entropy is 1, indicating a high level of impurity.\n",
    "\n",
    "It is a `measure` of `randomness/disorder/impurity`. It is used to `quantify` the `impurity/uncertainity`.\n",
    "\n",
    "- Entropy ranges from `0 to 1`.\n",
    "- `0` indicates as pure sub tree.\n",
    "- `1` indicates as most impure sub tree.\n",
    "\n",
    "## Gini Impurity\n",
    "\n",
    "Gini impurity is another measure of the impurity of a set of data, and is used in decision trees to determine the best split of the data. `For example`, if we have a set of data with two classes, and the classes are evenly distributed, then the Gini impurity is 0.5, indicating a moderate level of impurity.\n",
    "\n",
    "- How often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled.\n",
    "\n",
    "## Information Gain\n",
    "\n",
    "Information gain is a measure of the reduction in entropy or Gini impurity due to a split in a decision tree, and is used to determine the best split of the data. `For example`, if we have a set of data with two classes, and we split the data based on a feature that perfectly separates the classes, then the information gain is 1, indicating a perfect split.\n",
    "\n",
    "- Impure to Pure -> Information Gain increases.\n",
    "- Minimum entropy is on leaf node so, maximum information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./images/decision_treee.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
